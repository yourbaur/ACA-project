{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "developmental-vertical",
   "metadata": {},
   "source": [
    "Title: K-means clustering\n",
    "\n",
    "Authors: Bauyrzhan Taimanov - Madina Amanatayeva\n",
    "\n",
    "Date: Jan 18, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fresh-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-defeat",
   "metadata": {},
   "source": [
    "# 0) Context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-contents",
   "metadata": {},
   "source": [
    "## Customer Segmentation with Parallel K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-public",
   "metadata": {},
   "source": [
    "In business, we often say \"Customer is king\" and \"Customer is always right.\" Basically, it means we need to focus on what customers want. So, the idea here is to use this concept to group products that match what different groups of customers like. This can help a lot in online stores because it means showing customers the products they're most likely to be interested in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "durable-military",
   "metadata": {},
   "source": [
    "![alt text](https://editor.analyticsvidhya.com/uploads/73654cluster.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-honduras",
   "metadata": {},
   "source": [
    "This concept can be handy in places where we use digital marketing tools to promote products. For example, in some types of ads, we show products to people based on things like their interests. Then, by looking at whether they click on the ads or not, we can figure out who else might like those products.\n",
    "Similarly, in other types of ads, we bid for space to show our products to people in real-time. We do this based on what we know about the people seeing the ads and what's worked well in the past. This way, we can make sure we're showing the right products to the right people at the right time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-translation",
   "metadata": {},
   "source": [
    "## Concept\n",
    "\n",
    "Customers buy products based on different needs and constraints, like budget and preference for certain items. To understand these behaviors, we can analyze customer invoice data and other non-personal data.\n",
    "Here's a simplified approach:\n",
    "* Analyze customer invoice data to find buying patterns.\n",
    "* Use clustering algorithms to group customers based on similar purchase behaviors.\n",
    "* Identify products that match each customer group's preferences.\n",
    "* Tailor digital campaigns to each customer group for better targeting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-athens",
   "metadata": {},
   "source": [
    "Though, with our current data limitations, we may not have all the necessary details to build a complete model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-tribe",
   "metadata": {},
   "source": [
    "# 1) Analysis of the Serial Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-pavilion",
   "metadata": {},
   "source": [
    "Certainly! In customer segmentation, the goal is to group customers into distinct segments based on similarities in their behavior, preferences, or characteristics. K-Means clustering is a popular algorithm for this task. Here's how you can use it along with other algorithms to solve the customer segmentation problem:\n",
    "\n",
    "* K-Means Algorithm:\n",
    "Description: K-Means is an iterative clustering algorithm that aims to partition a dataset into K clusters.\n",
    "Usage in Customer Segmentation: You can use K-Means to cluster customers based on features such as purchase history, demographics, or behavioral data. By specifying the number of clusters (K), K-Means will assign each customer to the nearest centroid, creating distinct customer segments.\n",
    "Process:\n",
    "Choose the number of clusters (K).\n",
    "Initialize K centroids randomly.\n",
    "Assign each data point to the nearest centroid.\n",
    "Update the centroids by computing the mean of all data points assigned to each centroid.\n",
    "Repeat steps 3 and 4 until convergence or a specified number of iterations.\n",
    "Considerations: K-Means is sensitive to the initial placement of centroids and may converge to local optima. Therefore, it's essential to run the algorithm multiple times with different initializations and choose the best clustering solution based on evaluation metrics or domain knowledge.\n",
    "* Hierarchical Clustering:\n",
    "Description: Hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on similarity.\n",
    "Usage in Customer Segmentation: Hierarchical clustering can reveal hierarchical relationships among customer segments. It doesn't require specifying the number of clusters beforehand and can provide insights into the natural grouping structure of the data.\n",
    "Process:\n",
    "Start with each data point as a separate cluster.\n",
    "Merge the two closest clusters into a single cluster.\n",
    "Repeat step 2 until all data points are in one cluster or until a stopping criterion is met.\n",
    "Considerations: Hierarchical clustering can be computationally intensive for large datasets and may not scale well. Additionally, determining the optimal number of clusters can be subjective when using hierarchical clustering.\n",
    "* DBSCAN (Density-Based Spatial Clustering of Applications with Noise):\n",
    "Description: DBSCAN is a density-based clustering algorithm that groups together closely packed points based on density.\n",
    "Usage in Customer Segmentation: DBSCAN can identify dense regions of customers in the feature space, automatically detecting outliers as noise points.\n",
    "Process:\n",
    "Define two parameters: epsilon (a radius within which to search for neighboring points) and min_samples (the minimum number of points required to form a dense region).\n",
    "Identify core points (points with at least min_samples within distance epsilon) and border points (points within epsilon distance of a core point but with fewer than min_samples neighbors).\n",
    "Expand clusters by connecting core points to their neighbors and assigning border points to a cluster if they are reachable from a core point.\n",
    "Assign noise points to a separate cluster or mark them as outliers.\n",
    "Considerations: DBSCAN is robust to outliers and doesn't require specifying the number of clusters beforehand. However, it may struggle with clusters of varying densities and can be sensitive to the choice of epsilon and min_samples.\n",
    "* Gaussian Mixture Models (GMM):\n",
    "Description: GMM is a probabilistic model that assumes data points are generated from a mixture of several Gaussian distributions.\n",
    "Usage in Customer Segmentation: GMM can capture complex cluster shapes and model uncertainty in cluster assignments.\n",
    "Process:\n",
    "Initialize parameters for each Gaussian distribution (mean, covariance, and mixing coefficient).\n",
    "Expectation-Maximization (EM) algorithm: iteratively update parameters to maximize the likelihood of the observed data, assigning data points probabilities of belonging to each cluster.\n",
    "Cluster data points based on the highest probability of belonging to a particular cluster.\n",
    "Considerations: GMM can be computationally expensive, especially for high-dimensional data. It may also converge to local optima and requires careful initialization.\n",
    "Each of these algorithms has its strengths and weaknesses, and the choice depends on factors such as the nature of the data, the desired level of interpretability, and computational considerations. It's often helpful to experiment with multiple algorithms and evaluate their performance using metrics such as silhouette score, Daviesâ€“Bouldin index, or domain-specific criteria. Additionally, combining multiple algorithms or using ensemble methods can sometimes yield improved segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-shell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
